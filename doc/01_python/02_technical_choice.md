# Choix techniques

[Retour au menu](./00_menu.md)

- [Choix techniques](#choix-techniques)
  - [Stockage des donn√©es](#stockage-des-donn√©es)
  - [Impl√©mentation de la pipeline python](#impl√©mentation-de-la-pipeline-python)
    - [Structure du projet](#structure-du-projet)
  - [Importation des donn√©es](#importation-des-donn√©es)
  - [Nettoyage des donn√©es](#nettoyage-des-donn√©es)

## Stockage des donn√©es

Les donn√©es du projet sont enregistr√©es dans un dossier avec l'organisation suivante:

```
üìÇ data
‚î£ üìÇ 01_raw
‚î£ üìÇ 02_intermediate
‚îó üìÇ 03_result
```

Le dossier *01_raw* contient les donn√©es brutes telles qu'on les recoit. Dans le cadre d'un projet en production, cela correspondrait √† un **data lake**.

Le dossier *02_intermediate* contient les r√©sultats interm√©diaires. On peut par exemple exporter certaines tables ayant re√ßu un premier traitement.

Enfin le dossier *03_result* contient la donn√©e enti√®rement pr√©par√©e telle qu'elle est demand√©e dans l'√©nonc√©.

## Impl√©mentation de la pipeline python

Afin de coder le pipeline de traitement de donn√©es en python, il faut s√©lectionner un framework adapt√© pour la manipulation de donn√©es. Plusieurs choix sont possibles en Python. J'ai opt√© pour la librairie **pandas**. Cette librairie permet d'importer et de manipuler des dataframes. Ce choix est adapt√© √† la taille des jeux de donn√©es. Cependant, ce choix serait moins pertinent avec une mise en production sur des jeux de donn√©es plus volumineux. Il serait pr√©ferable dans ce cas de figure de choisir un framework comme **Spark**.

### Structure du projet

Le projet est structur√© de la mani√®re suivante:

```
üì¶ package
‚î£ üìÇ data
‚îÉ ‚îó üìÇ load
‚îÉ ‚îó üìÇ transform
‚îó üìÇ processing
```

Le projet a √©t√© pens√© de mani√®re √† r√©pondre √† plusieurs besoins:

- Le projet est packag√© pour √™tre utilis√© facilement
- Le package est d√©coup√© en sous module pouvant √™tre utilis√© ind√©pendament. Cela permet de r√©utiliser les modules pour d'autres traitements. Il est aussi possible de controler ces modules dans un pipeline √† l'aide d'un outil comme Airflow.

> [!NOTE]
> Dans mon projet, j'ai utilis√© la feature *pipe* de **pandas** pour construire ma pipeline.


> [!NOTE]  
> Le module "data/transform" a √©t√© test√© √† l'aide du framework .

## Importation des donn√©es

Le module d'importation des donn√©es doit r√©pondre √† plusieurs besoins:

- Il doit supporter plusieurs types de donn√©es en entr√©e: CSV et JSON
- Il doit s'assurer de l'int√©grit√© des donn√©es en entr√©e (virgule en trop dans le JSON par exemple)
- Il doit retourner toutes les donn√©es sous forme de dataframes

## Nettoyage des donn√©es

Avant de pouvoir travailler sur les donn√©es afin de construire le JSON de sortie, on doit s'assurer que les donn√©es soient propres. Pour cela, j'ai identifi√© les points suivants:

- Certains string pr√©sentent des artefacts du au format UTF-8. Il faudrait nettoyer les chaines de caract√®res pour les retirer.
- Les donn√©es pr√©sentes des valeurs manquantes, il faudrait id√©alement inf√©rer les valeurs manquantes (id) ou les remplacer par des NA

Lorsque les donn√©es seront propres, une derni√®re √©tape du nettoyage pourra consister √† proprement typer les colonnes.
